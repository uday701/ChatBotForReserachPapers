{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Run the grobid server locally and convert the pdf to xml files using grobid client.\n"
      ],
      "metadata": {
        "id": "InsxtW9zxnH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!grobid_client --input \"/content/drive/MyDrive/dataset\" --output \"/content/drive/MyDrive/datasettei\""
      ],
      "metadata": {
        "id": "L2f_4hzJxjEh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Xml Files"
      ],
      "metadata": {
        "id": "h2CSLGI7yHkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grobid_tei_xml\n",
        "!pip install dateparser\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RMqWbeX0bhL",
        "outputId": "12439021-77e8-42f7-c8e3-88a182591702"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: grobid_tei_xml in /usr/local/lib/python3.10/dist-packages (0.1.3)\n",
            "Requirement already satisfied: dateparser in /usr/local/lib/python3.10/dist-packages (1.2.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from dateparser) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser) (2023.4)\n",
            "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser) (2023.12.25)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser) (5.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->dateparser) (1.16.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "from bs4 import BeautifulSoup\n",
        "import dateparser\n",
        "import grobid_tei_xml\n",
        "from pathlib import Path\n",
        "\n",
        "import tiktoken\n",
        "import copy\n",
        "import json"
      ],
      "metadata": {
        "id": "xf6pVnug0GRr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the dataset folder\n",
        "dataset_folder = \"/content/drive/MyDrive/datasettei/\"\n",
        "\n",
        "# Define the output parent folder\n",
        "output_parent_folder = \"/content/drive/MyDrive/llama-2/\""
      ],
      "metadata": {
        "id": "s-zEoWoVt9Sq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_grobid_xml(text):\n",
        "    output_data = OrderedDict()\n",
        "\n",
        "    doc_biblio = grobid_tei_xml.parse_document_xml(text)\n",
        "    biblio = {\n",
        "        \"doi\": doc_biblio.header.doi if doc_biblio.header.doi is not None else \"\",\n",
        "        \"authors\": \", \".join([author.full_name for author in doc_biblio.header.authors]),\n",
        "        \"title\": doc_biblio.header.title,\n",
        "        \"hash\": doc_biblio.pdf_md5\n",
        "    }\n",
        "    try:\n",
        "        year = dateparser.parse(doc_biblio.header.date).year\n",
        "        biblio[\"publication_year\"] = year\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    output_data['biblio'] = biblio\n",
        "    passages = []\n",
        "    output_data['passages'] = passages\n",
        "    passage_type = \"paragraph\"\n",
        "\n",
        "    soup = BeautifulSoup(text, 'xml')\n",
        "    blocks_header = get_xml_nodes_header(soup, use_paragraphs=True)\n",
        "\n",
        "    \"\"\"passages.append({\n",
        "        \"text\": f\"authors: {biblio['authors']}\",\n",
        "       \"type\": passage_type,\n",
        "        \"section\": \"<header>\",\n",
        "        \"subSection\": \"<title>\",\n",
        "        \"passage_id\": \"htitle\",\n",
        "    })\n",
        "\n",
        "    passages.append({\n",
        "        \"text\": post_process(\" \".join([node.text for node in blocks_header['title']])),\n",
        "        \"type\": passage_type,\n",
        "        \"section\": \"<header>\",\n",
        "        \"subSection\": \"<title>\",\n",
        "        \"passage_id\": \"htitle\",\n",
        "    })\"\"\"\n",
        "\n",
        "    passages.append({\n",
        "        \"text\": post_process(\n",
        "            ''.join(node.text for node in blocks_header['abstract'] for text in node.find_all(text=True) if\n",
        "                    text.parent.name != \"ref\" or (\n",
        "                            text.parent.name == \"ref\" and text.parent.attrs[\n",
        "                        'type'] != 'bibr'))),\n",
        "        \"type\": passage_type,\n",
        "        \"section\": \"<header>\",\n",
        "        \"subSection\": \"<abstract>\",\n",
        "        \"passage_id\": \"habstract\",\n",
        "    })\n",
        "\n",
        "    text_blocks_body = get_xml_nodes_body(soup, verbose=False, use_paragraphs=True)\n",
        "\n",
        "    use_paragraphs = True\n",
        "    if not use_paragraphs:\n",
        "        passages.extend([\n",
        "            {\n",
        "                \"text\": post_process(''.join(text for text in sentence.find_all(text=True) if\n",
        "                                                  text.parent.name != \"ref\" or (\n",
        "                                                          text.parent.name == \"ref\" and text.parent.attrs[\n",
        "                                                      'type'] != 'bibr'))),\n",
        "                \"type\": passage_type,\n",
        "                \"section\": \"<body>\",\n",
        "                \"subSection\": \"<paragraph>\",\n",
        "                \"passage_id\": str(paragraph_id),\n",
        "            }\n",
        "            for paragraph_id, paragraph in enumerate(text_blocks_body) for\n",
        "            sentence_id, sentence in enumerate(paragraph)\n",
        "        ])\n",
        "    else:\n",
        "        passages.extend([\n",
        "            {\n",
        "                \"text\": post_process(''.join(text for text in paragraph.find_all(text=True) if\n",
        "                                                  text.parent.name != \"ref\" or (\n",
        "                                                          text.parent.name == \"ref\" and text.parent.attrs[\n",
        "                                                      'type'] != 'bibr'))),\n",
        "                \"type\": passage_type,\n",
        "                \"section\": \"<body>\",\n",
        "                \"subSection\": \"<paragraph>\",\n",
        "                \"passage_id\": str(paragraph_id),\n",
        "            }\n",
        "            for paragraph_id, paragraph in enumerate(text_blocks_body)\n",
        "        ])\n",
        "\n",
        "    text_blocks_figures = get_xml_nodes_figures(soup, verbose=False)\n",
        "\n",
        "    if not use_paragraphs:\n",
        "        passages.extend([\n",
        "            {\n",
        "                \"text\": post_process(''.join(text for text in sentence.find_all(text=True) if\n",
        "                                                  text.parent.name != \"ref\" or (\n",
        "                                                          text.parent.name == \"ref\" and text.parent.attrs[\n",
        "                                                      'type'] != 'bibr'))),\n",
        "                \"type\": passage_type,\n",
        "                \"section\": \"<body>\",\n",
        "                \"subSection\": \"<figure>\",\n",
        "                \"passage_id\": str(paragraph_id) + str(sentence_id),\n",
        "            }\n",
        "            for paragraph_id, paragraph in enumerate(text_blocks_figures) for\n",
        "            sentence_id, sentence in enumerate(paragraph)\n",
        "        ])\n",
        "    else:\n",
        "        passages.extend([\n",
        "            {\n",
        "                \"text\": post_process(''.join(text for text in paragraph.find_all(text=True) if\n",
        "                                                  text.parent.name != \"ref\" or (\n",
        "                                                          text.parent.name == \"ref\" and text.parent.attrs[\n",
        "                                                      'type'] != 'bibr'))),\n",
        "                \"type\": passage_type,\n",
        "                \"section\": \"<body>\",\n",
        "                \"subSection\": \"<figure>\",\n",
        "                \"passage_id\": str(paragraph_id),\n",
        "            }\n",
        "            for paragraph_id, paragraph in enumerate(text_blocks_figures)\n",
        "        ])\n",
        "\n",
        "    return output_data\n"
      ],
      "metadata": {
        "id": "adnYONO7yHAK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_xml_nodes_header(soup: object, use_paragraphs: bool = True) -> list:\n",
        "    sub_tag = \"p\" if use_paragraphs else \"s\"\n",
        "\n",
        "    header_elements = {\n",
        "        \"authors\": [persNameNode for persNameNode in soup.teiHeader.find_all(\"persName\")],\n",
        "        \"abstract\": [p_in_abstract for abstractNodes in soup.teiHeader.find_all(\"abstract\") for p_in_abstract in\n",
        "                     abstractNodes.find_all(sub_tag)],\n",
        "        \"title\": [soup.teiHeader.fileDesc.title]\n",
        "    }\n",
        "\n",
        "    return header_elements\n",
        "\n",
        "\n",
        "def get_xml_nodes_body(soup: object, use_paragraphs: bool = True, verbose: bool = False) -> list:\n",
        "    nodes = []\n",
        "    tag_name = \"p\" if use_paragraphs else \"s\"\n",
        "    for child in soup.TEI.children:\n",
        "        if child.name == 'text':\n",
        "            # nodes.extend([subchild.find_all(tag_name) for subchild in child.find_all(\"body\")])\n",
        "            nodes.extend(\n",
        "                [subsubchild for subchild in child.find_all(\"body\") for subsubchild in subchild.find_all(tag_name)])\n",
        "\n",
        "    if verbose:\n",
        "        print(str(nodes))\n",
        "\n",
        "    return nodes\n",
        "\n",
        "\n",
        "def get_xml_nodes_figures(soup: object, verbose: bool = False) -> list:\n",
        "    children = []\n",
        "    for child in soup.TEI.children:\n",
        "        if child.name == 'text':\n",
        "            children.extend(\n",
        "                [subchild for subchilds in child.find_all(\"body\") for subchild in subchilds.find_all(\"figDesc\")])\n",
        "\n",
        "    if verbose:\n",
        "        print(str(children))\n",
        "\n",
        "    return children\n",
        "\n",
        "def post_process( text):\n",
        "        output = text.replace('À', '-')\n",
        "        output = output.replace('¼', '=')\n",
        "        output = output.replace('þ', '+')\n",
        "        output = output.replace('Â', 'x')\n",
        "        output = output.replace('$', '~')\n",
        "        output = output.replace('−', '-')\n",
        "        output = output.replace('–', '-')\n",
        "        patterns = [\n",
        "        r'\\d+e\\d+'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            output = re.sub(pattern, lambda match: match.group().replace('e', '-'), output)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "1-Saj8GU1RS4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def encode( text, allowed_special=set(), disallowed_special=\"all\"):\n",
        "  return enc.encode(\n",
        "      text,\n",
        "      allowed_special=allowed_special,\n",
        "      disallowed_special=disallowed_special,\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "ggRCvVJR3uno"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_passages(passages, chunk_size, tolerance=0.2):\n",
        "    new_passages = []\n",
        "    current_texts = []\n",
        "\n",
        "    for idx, passage in enumerate(passages):\n",
        "        text = passage['text']\n",
        "        current_texts.append(text)\n",
        "\n",
        "        accumulated_text = \" \".join(current_texts)\n",
        "        encoded_accumulated_text = encode(accumulated_text)\n",
        "\n",
        "        if len(encoded_accumulated_text) > chunk_size + chunk_size * tolerance:\n",
        "            if len(current_texts) > 1:\n",
        "                new_passages.append(current_texts[:-1])\n",
        "                current_texts = [current_texts[-1]]\n",
        "            else:\n",
        "                new_passages.append(current_texts)\n",
        "                current_texts = []\n",
        "\n",
        "        elif chunk_size <= len(encoded_accumulated_text) < chunk_size + chunk_size * tolerance:\n",
        "            new_passages.append(current_texts)\n",
        "            current_texts = []\n",
        "\n",
        "    if len(current_texts) > 0:\n",
        "        new_passages.append(current_texts)\n",
        "\n",
        "    new_passages_struct = []\n",
        "\n",
        "    for i, passages in enumerate(new_passages):\n",
        "        text = \" \".join(passages)\n",
        "\n",
        "        new_passages_struct.append(\n",
        "            {\n",
        "                \"text\": text,\n",
        "                \"type\": \"aggregated chunks\",\n",
        "                \"section\": \"mixed\",\n",
        "                \"subSection\": \"mixed\"\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return new_passages_struct\n"
      ],
      "metadata": {
        "id": "mrcSrva04JAc"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create output parent folder if it doesn't exist\n",
        "os.makedirs(output_parent_folder, exist_ok=True)\n",
        "\n",
        "# Iterate through each file in the dataset folder\n",
        "for file_path in Path(dataset_folder).glob(\"*.tei.xml\"):\n",
        "    text = \"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Parse the XML and perform other operations\n",
        "    output = parse_grobid_xml(text)\n",
        "\n",
        "    # Extract relevant information\n",
        "    output['filename'] = file_path\n",
        "    biblio = output['biblio']\n",
        "    filename = file_path.stem\n",
        "    biblio['filename'] = filename.replace(\" \", \"_\")\n",
        "\n",
        "    # Create a separate folder for each file\n",
        "    output_folder = os.path.join(output_parent_folder, f\"{filename}_output\")\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    texts = []\n",
        "    metadatas = []\n",
        "    ids = []\n",
        "\n",
        "    new_passages = merge_passages(output['passages'], chunk_size=250)\n",
        "\n",
        "    # Iterate through each passage in the new_passages\n",
        "    for passage_id, passage in enumerate(new_passages):\n",
        "        biblio_copy = copy.copy(biblio)\n",
        "\n",
        "        if len(str.strip(passage['text'])) > 0:\n",
        "            texts.append(passage['text'])\n",
        "\n",
        "            biblio_copy['type'] = passage['type']\n",
        "            biblio_copy['section'] = passage['section']\n",
        "            biblio_copy['subSection'] = passage['subSection']\n",
        "            metadatas.append(biblio_copy)\n",
        "\n",
        "            ids.append(passage_id)\n",
        "\n",
        "    # Store the results in separate files within the output folder\n",
        "    output_text_path = os.path.join(output_folder, f\"{filename}_text.txt\")\n",
        "    output_metadata_path = os.path.join(output_folder, f\"{filename}_metadata.json\")\n",
        "\n",
        "    with open(output_text_path, 'w', encoding='utf-8') as text_file:\n",
        "        text_file.write(\"\\n\".join(texts))\n",
        "\n",
        "    with open(output_metadata_path, 'w', encoding='utf-8') as metadata_file:\n",
        "        json.dump(metadatas, metadata_file, indent=2)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaMSMYkAD-g3",
        "outputId": "f3437b6b-dfeb-4a14-ccdc-5c3e1c3a65ae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-5c0c7f4d4c6b>:43: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  ''.join(node.text for node in blocks_header['abstract'] for text in node.find_all(text=True) if\n",
            "<ipython-input-37-5c0c7f4d4c6b>:74: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  \"text\": post_process(''.join(text for text in paragraph.find_all(text=True) if\n",
            "<ipython-input-37-5c0c7f4d4c6b>:106: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  \"text\": post_process(''.join(text for text in paragraph.find_all(text=True) if\n"
          ]
        }
      ]
    }
  ]
}